x[1:15]
x[1:15,]
miss
x$col[miss] <- "red"
x$cool
x
x$col
x$pch <- 1
x$pch[miss] <- 19
plot(x$solar, x$wind, col=x$col, pch=x$pch, xlab='solar', ylab='ozone')
x$col <- null
x$col <- NULL
x$pch <- NULL
x
mosaic(table(x[,5:6]))
osaic(table(x[,5:6]))
mosaic(table(x[,5:6]))
boxplot(x$ozone ~ x$month, xlab="Month", ylab="Ozone", main="Ozone by Month")
model <- lm(ozone~.-day, x)
summary(model)
plot(x$ozone, type="b", xlab='Days, Counting', ylab='Ozone', main='NYC Ozone Time Series')
plot(x$ozone, type="s", xlab='Days, Counting', ylab='Ozone', main='NYC Ozone Time Series')
plot(x$ozone, type="a", xlab='Days, Counting', ylab='Ozone', main='NYC Ozone Time Series')
plot(x$ozone, type="c", xlab='Days, Counting', ylab='Ozone', main='NYC Ozone Time Series')
miss <- which(is.na(x$ozone))
not.miss <- which(!is.na(x$ozone))
z <- matrix(NA, nrow=length(miss), ncol=4)
colnames(z) <- c("value1", "value2", "index1", "index2")
rownames(z) <- miss
z <- data.frame(z)
miss
not.miss
z <- matrix(NA, nrow=length(miss), ncol=4)
z
colnames(z) <- c("value1", "value2", "index1", "index2")
rownames(z) <- miss
z
z <- data.frame(z)
z
yaletoolkit
install(yaletoolkit)
dir
ls()
pwd
pwd()
dir()
print("I love my pc.")
?rnorm
sample(1:0, 161)
sample(1:0, 161, replace=T)
# --- Linear Classification Methods
# *Linear Regression of an Indicator Matrix
# Generate Data
library(MASS)
n1 <- 200
n2 <- 200
n3 <- 200
sigma<-.05
c1 <- mvrnorm(n1, c(-1,-1), diag(rep(sigma,2)))
c2 <- mvrnorm(n2, c(0,0), diag(rep(sigma,2)))
c3 <- mvrnorm(n3, c(1,1), diag(rep(sigma,2)))
plot(rbind(c1,c2,c3))
g <- c(rep(1,n1),rep(2,n2), rep(3,n3))
data <- data.frame(cbind(rbind(c1,c2,c3),g))
names(data) <- c("X1","X2","Class")
plot(data[,1:2],pch=data[,3])
y1 <- as.numeric(data[,3]==1)
y2 <- as.numeric(data[,3]==2)
y3 <- as.numeric(data[,3]==3)
y1.hat <- predict(lm(y1~X1+X2,data),data)
y2.hat <- predict(lm(y2~X1+X2,data),data)
y3.hat <- predict(lm(y3~X1+X2,data),data)
yhat <- cbind(y1.hat, y2.hat, y3.hat)
apply(yhat,1,sum)
# The Masking Problem
g.hat <- apply(yhat,1,order)[3,]
plot(data[,1:2],pch=g.hat)
#matplot(cbind(y1.hat,y2.hat,y3.hat))
### Linear Discriminant Analysis (LDA)
library(MASS)
data[,3] <- as.factor(data[,3])
z <- lda(Class ~ X1+X2, data)
yhat <- predict(z,data[,1:2])$class
plot(data[,1:2],col=as.numeric(yhat))
table(g, yhat)
s1 <- cov(data[g==1,1:2])*(n1-1)
s1
data[g==1,1:2]
data[g==2,1:2]
data[g==3,1:2]
cov(data[g==1,1:2])
cov(data[g==1,1:2])
cov(data[g==1,1:2])
cov(data[g==1,1:2])
cov(data[g==1,1:2])
cov(data[g==1,1:2])
cov(data[g==1,1:2])
cov(data[g==1,1:2])
cov(data[g==1,1:2])
cov(data[g==1,1:2])
cov(data[g==1,1:2])
cov(data[g==1,1:2])
cov(data[g==1,1:2])
cov(data[g==1,1:2])
cov(data[g==1,1:2])
cov(data[g==1,1:2])
cov(data[g==1,1:2])
[g==1,1:2])*(n1-1)
s1 <- cov(data[g==1,1:2])*(n1-1)
s2 <- cov(data[g==2,1:2])*(n2-1)
s3 <- cov(data[g==3,1:2])*(n3-1)
sgm <- (s1+s2+s3)/(n1+n2+n3-3)
sgm
mu1 <- mean(data[g==1,1:2])
mu2 <- mean(data[g==2,1:2])
mu3 <- mean(data[g==3,1:2])
mu1
mu2
mu3
pc2 <- prcomp(data[,1:2])
pc2
windows()
plot(pc2$x,col=as.numeric(data[,3]), xlim=range(pc2$x), ylim=range(pc2$x))
matplot(pc2$x[,1],cbind(g1hat, g2hat, g3hat),pch=16)
# --- Linear Classification Methods
# *Linear Regression of an Indicator Matrix
# Generate Data
library(MASS)
n1 <- 200
n2 <- 200
n3 <- 200
sigma<-.05
c1 <- mvrnorm(n1, c(-1,-1), diag(rep(sigma,2)))
c2 <- mvrnorm(n2, c(0,0), diag(rep(sigma,2)))
c3 <- mvrnorm(n3, c(1,1), diag(rep(sigma,2)))
plot(rbind(c1,c2,c3))
g <- c(rep(1,n1),rep(2,n2), rep(3,n3))
data <- data.frame(cbind(rbind(c1,c2,c3),g))
names(data) <- c("X1","X2","Class")
plot(data[,1:2],pch=data[,3])
y1 <- as.numeric(data[,3]==1)
y2 <- as.numeric(data[,3]==2)
y3 <- as.numeric(data[,3]==3)
y1.hat <- predict(lm(y1~X1+X2,data),data)
y2.hat <- predict(lm(y2~X1+X2,data),data)
y3.hat <- predict(lm(y3~X1+X2,data),data)
yhat <- cbind(y1.hat, y2.hat, y3.hat)
apply(yhat,1,sum)
# The Masking Problem
g.hat <- apply(yhat,1,order)[3,]
plot(data[,1:2],pch=g.hat)
#matplot(cbind(y1.hat,y2.hat,y3.hat))
### Linear Discriminant Analysis (LDA)
library(MASS)
data[,3] <- as.factor(data[,3])
z <- lda(Class ~ X1+X2, data)
yhat <- predict(z,data[,1:2])$class
plot(data[,1:2],col=as.numeric(yhat))
table(g, yhat)
# keep track of the linear discriminant functions
s1 <- cov(data[g==1,1:2])*(n1-1)
s2 <- cov(data[g==2,1:2])*(n2-1)
s3 <- cov(data[g==3,1:2])*(n3-1)
sgm <- (s1+s2+s3)/(n1+n2+n3-3)
mu1 <- mean(data[g==1,1:2])
mu2 <- mean(data[g==2,1:2])
mu3 <- mean(data[g==3,1:2])
ldf <- function(x, mu, sigma, pi)
{
sigma.invs <- solve(sigma) # not computationally efficient
return(x%*%sigma.invs%*%mu - 1/2*mu%*%sigma.invs%*%mu + log(pi))
}
g1hat <-
apply(as.matrix(data[,1:2]), 1, function(x) {return(ldf(x, mu1, sgm, 1/3))})
g2hat <-
apply(as.matrix(data[,1:2]), 1, function(x) {return(ldf(x, mu2, sgm, 1/3))})
g3hat <-
apply(as.matrix(data[,1:2]), 1, function(x) {return(ldf(x, mu3, sgm, 1/3))})
pc2 <- prcomp(data[,1:2])
windows()
plot(pc2$x,col=as.numeric(data[,3]), xlim=range(pc2$x), ylim=range(pc2$x))
matplot(pc2$x[,1],cbind(g1hat, g2hat, g3hat),pch=16)
library(lars)
library(MASS)
results = matrix(NA,nrow=30,ncol=7)
results.train = matrix(NA,nrow=30,ncol=7)
results.nvars = matrix(NA,nrow=30,ncol=7)
results.lams = matrix(NA,nrow=30,ncol=2)
colnames(results.nvars) = colnames(results.train) = colnames(results) = c("Simple Lm","Stepwise AIC backward","Stepwise BIC backward","Stepwise AIC forward","Stepwise BIC forward","Ridge Regression with CV","Lasso with CV")
colnames(results.lams) = c("RR.lams","Lasso.Lams")
?klar
library("kklar")
library("klar")
library(rpart)
# Simulated data: Nested Sphere (eq 10.2)
n0 <- 2000
n <- 1000
p <- 10 # make change of the dimension to see the  effect of Curse of Dimensionality
x <- matrix(rnorm(n*p), n0, p)
crit <- qchisq(p=0.5, df=p)
x2sum <- apply(x^2, 1, sum)
y <- ifelse(x2sum>crit, 1,0)
xy <- data.frame(cbind(x,y)[1:n,])
xy.test <- data.frame(cbind(x,y)[(n+1):n0,])
names(xy) <- c(paste("X", 1:p,sep=""),"Y")
names(xy.test) <- c(paste("X", 1:p,sep=""),"Y")
#plot(X2 ~ X1, data=xy,col=y+1)
y.tr <- xy[,p+1]
y.test <- xy.test[,p+1]
# Adaboost M1
w <- rep(1/1000, 1000)
ghat <- rep(0,n) #classification result from comittee
ghat.test <- rep(0,n)
M <- 100 # number of iterations
# to store the results after each iteration
w.m <- matrix(0,M,n)
misInd.m <- matrix(0,M,n)#For Individual Classifier
misInd.m.test <- matrix(0,M,n)#For Individual Classifier
misInd1.m <- matrix(0,M,n)#For Committee
misInd1.m.test <- matrix(0,M,n)#For Committee
err.v <- rep(0,M)# For Individual Classifier
alpha.v <- rep(0,M)
i <- 1
for( i in 1:M)
{
w.m[i,] <- w
tr <- rpart(Y~., data=xy, weights=w, method="class",maxdepth=1)
phat<-predict(tr,xy)
yhat <- ifelse(phat[,1]>.5, 0, 1)
misInd <- ifelse(yhat+y.tr==1,1,0) #Misclassification Indicator of an individual classifier
err <- sum(w*misInd)/sum(w)
alpha <- log((1-err)/err)
w <- w*exp(alpha*misInd)
ghat <- ghat + alpha*(yhat-1/2)
yhat1 <- ifelse(ghat>0,1,0)
misInd1 <- ifelse(yhat1+y.tr==1,1,0) #Misclassification Indicator of the committee
misInd.m[i,] <- misInd
misInd1.m[i,]<- misInd1
err.v[i] <- err
alpha.v[i] <- alpha
phat.test <- predict(tr,xy.test)
yhat.test <- ifelse(phat.test[,1]>.5, 0, 1)
misInd.test <- ifelse(yhat.test+y.test==1,1,0)
ghat.test <- ghat.test + alpha*(yhat.test-1/2)
yhat1.test <- ifelse(ghat.test>0,1,0)
misInd1.test <- ifelse(yhat1.test+y.test==1,1,0) #Misclassification Indicator of the committee
misInd.m.test[i,] <- misInd.test
misInd1.m.test[i,]<- misInd1.test
#    par(mfrow=c(1,2))
#        plot(X2~X1,data=xy, col=misInd+1,pch=16)
#        plot(X2~X1,data=xy, col=misInd1+1,pch=16)
#    par(mfrow=c(1,1))
#    Sys.sleep(3)
}
#plot(x[,7], x[,1],col=y+1)
#abline(v=tr$split[1,4)]
w.sum <- apply(w.m,1,sum)
w.m1 <- w.m/(w.sum%*%t(rep(1,n)))
par(mfrow=c(4,2),mar=c(3,3,1,1),mgp=c(2,1,0))
for(i in 1:4)
{
plot(xy$X9,w.m1[i,]+rnorm(n,sd=.00001),col=misInd.m[i,]+1,ylim=range(w.m1[1:4,]),ylab="weight",main=paste("iter",i))
plot(xy$X9,w.m1[i+1,]+rnorm(n,sd=.00001),col=misInd.m[i,]+1,ylim=range(w.m1[1:4,]),ylab="")
}
par(mfrow=c(1,1))
par(mfrow=c(2,2))
error <- apply(misInd.m, 1, sum)
plot(error/n, type="l",ylab="training error", ylim=c(0.4,0.55),main="Individual Classifier")
error1 <- apply(misInd1.m, 1, sum)
plot(error1/n, type="l",ylab="training error", ylim=c(0.1,0.5),main="Committee")
error <- apply(misInd.m.test, 1, sum)
plot(error/n, type="l",ylab="test error",ylim=c(0.4,0.55),main="Individual Classifier")
error1 <- apply(misInd1.m.test, 1, sum)
plot(error1/n, type="l",ylab="test error",ylim=c(0.1,0.5),main="Committee")
par(mfrow=c(1,1))
par(mfrow=c(1,1))
?GBM
library("GBM")
install.packages("GBM")
data <- evars1[complete.cases(evars3),]
data <- evars1[complete.cases(evars1),]
# Get the index in FRAME that is LAGSEC lagged behind TIME, starting to look from START.  LAGSEC is a difftime class (see as.difftime(x,units="y"))).  Let FRAME be the full data frame we're searching through.
getLagIndex = function(frame,time,lagsec,start) {
# Create placeholder vectors.
ftime = frame$time
last = start
# iterate through the array.
for (i in (start+1):length(ftime)) {
# If the frame's time is NA, do nothing.  There are many NAs in our data.
if (!is.na(ftime[i])) {
# If the frame's time is already ahead of the time we want, skip it.  All times are chronological, so once we find a single frame time ahead of our time, we can go straight to NA.
if (as.double(time - ftime[i],units="secs") > 0) {
# If the frame's time is behind time, keep iterating until we find a time such that the last non-NA time is too far lagged and this time is lagged enough.  Return that time.
if ((as.double(time - ftime[last],units="secs") > lagsec) & (as.double(time - ftime[i],units="secs") <= as.double(lagsec,units="secs"))) {
return(i)
}
# If ever we get past time, return NA because no later frame times will be correct.
} else {
return(NA)
}
# As long as ftime[i] was not NA, we should remember it as the last non-NA time index.
last = i
}
}
# If somehow we reach the end of the array without finding a good time, return NA.
return(NA)
}
# Let FRAME be the full frame of things we're searching through. Let LAG be a difftime object created by as.difftime(x,units="y")
# ADDED var to getFutureLagged - to select variable from frame
getFutureLagged = function(futures,frame,lag,var) {
# Create placeholder vectors.
indepTime = futures$time
fulag1 = rep(NA,length(indepTime))
indices = rep(NA,length(indepTime))
# Iterate through the array.
for (i in 1:length(indepTime)) {
# Let the lagged index for i be given by getLagIndex.  Start the search at one before the last index for which we found something.
newInd = getLagIndex(frame,indepTime[i],lag,max(2,indices,na.rm=TRUE)-1)
# If no lagged index was found, let index be the value of the last known good index to start from, or let it be 1 if there is no last known good index.
if (is.na(newInd)) {
indices[i] = NA
fulag1[i] = NA
# If we found a lagged index, let fulag[i] be the close price at that index, and let it be known that the next solution will be found near here.
} else {
indices[i] = newInd
fulag1[i] = frame[newInd,var]
}
}
return(fulag1)
}
# Seems to work.
getStockLagged = function(futures,stocknumber,lag) {
frame = eval(parse(text=paste("stock",stocknumber,sep="")))
laggedStock = getFutureLagged(futures,frame,lag)
return(laggedStock)
}
# Gets the best high in FRAME in the past LOOKBACK minutes, where LOOKBACK is a difftime object.
getFutureBestHigh = function(futures,frame,lookback) {
indepTime = futures$time
besthigh = rep(NA,length(indepTime))
indices = rep(NA,length(indepTime))
for (i in 1:length(indepTime)) {
firstCheck = getLagIndex(frame,indepTime[i],lookback,max(2,indices,na.rm=TRUE)-1)
potentialList = c()
if (!is.na(firstCheck)) {
indices[i] = firstCheck
while (indepTime[i] > frame$time[firstCheck]) {
potentialList = c(potentialList,frame$high[firstCheck])
firstCheck = firstCheck + 1
}
besthigh[i]=max(potentialList,na.rm=TRUE)
} else {
indices[i]=NA
besthigh[i]=NA
}
}
return(besthigh)
}
getFutureWorstLow = function(futures,frame,lookback) {
indepTime = futures$time
worstlow = rep(NA,length(indepTime))
indices = rep(NA,length(indepTime))
for (i in 1:length(indepTime)) {
firstCheck = getLagIndex(frame,indepTime[i],lookback,max(2,indices,na.rm=TRUE)-1)
potentialList = c()
if (!is.na(firstCheck)) {
indices[i] = firstCheck
while (indepTime[i] > frame$time[firstCheck]) {
potentialList = c(potentialList,frame$low[firstCheck])
firstCheck = firstCheck + 1
}
worstlow[i]=min(potentialList,na.rm=TRUE)
} else {
indices[i]=NA
worstlow[i]=NA
}
}
return(worstlow)
}
getFutureVolLagged = function(futures,frame,lag) {
indepTime = futures$time
fulag1 = rep(NA,length(indepTime))
indices = rep(NA,length(indepTime))
for (i in 1:length(indepTime)) {
newInd = getLagIndex(frame,indepTime[i],lag,max(2,indices,na.rm=TRUE)-1)
if (is.na(newInd)) {
indices[i] = NA
fulag1[i] = NA
} else {
indices[i] = newInd
fulag1[i] = frame$volume[newInd]
}
}
return(fulag1)
}
# Type YEAR gives fraction of the year (0 to 355/365), type MONTH gives week of the month (i.e. 0, 1, 2, 3, or 4), any other type returns fraction of day (0 to 23/24).
getTimeMarker = function(frame,type) {
if (type=="year") {
return(as.numeric(strftime(frame$time,format="%j")) / 365)
} else if (type=="month") {
return(as.numeric(strftime(frame$time,format="%W")) %% 4)
} else {
return(as.numeric(strftime(frame$time,format="%H")) / 24)
}
}
# Where family is either linear or binomial
# Returns either average missclassification or
glm.cv(evars3, 123, "response ~ .", "binomial", "binomial", 10)
lm.cv = function(data, seed, formula, response, k) {
args = c(formula=formula)
res = cv(data=data, seed=seed, k=k, response=response, func="lm", args=args)
return(res)
}
glm.cv = function(data, seed, formula, response, family, k) {
args = c(formula=formula, family=family)
res = cv(data=data, seed=seed, k=k, response=response, func="glm", args=args)
return(res)
}
library("randomForest")
rf.cv = function(data, seed, formula, response, k) {
args = c(formula=formula, ntree=500, mtry=8, nodesize=5)
res = cv(data=data, seed=seed, k=k, response=response, func="randomForest", args=args)
}
cv = function(data, seed, k, response, func, args) {
data <- data[complete.cases(data),]
n= nrow(data); p=ncol(data)
set.seed(seed)
id <- sample(1:n);data1=data[id,]
group <- rep(1:k, n/k+1)[1:n]
test.error <- rep(0, k)
for(i in 1:k) {
test <- data1[group==i,]
train <- data1[group!=i,]
if(func=="randomForest") {
m <- randomForest(response~.,data=train, ntree=as.numeric(args["ntree"]), mtry=as.numeric(args["mtry"]), nodesize=as.numeric(args["nodesize"]), keep.forest=TRUE)
}
else {
argsWithData <- list(args, data=train)
m <- do.call(func, argsWithData)
}
m.yhat <- predict(m, test)
nas <- as.numeric(na.action(na.omit(m.yhat)))
nas.2 <- na.action(na.omit(test$response))
m.yhat <- m.yhat[-unique(nas,nas.2)]
test <- test[-unique(nas,nas.2),]
if(response == "linear") {
test.error[i] <- sqrt(sum((m.yhat-test$response)^2)/nrow(test))
}
if(response == "binomial") {
m.yhat <- as.numeric(m.yhat>.5)
tab <- table(test$response, m.yhat)
test.error[i] <- 1-sum(diag(tab))/sum(tab)
}
}
return(mean(test.error))
}
######################################
######## READ IN DATA ################
######################################
# Read data from csv files.  Note that Excel was converted to csv separately.
setwd("/Users/AKumar/Documents/Yale Year Two/Yale Spring 2013/STAT 365/Final Project/Raw Data/")
setwd("C:/Development/Sites/365final/Raw Data/")
for (i in 1:300) {
eval(parse(text=paste("stock",i," = read.csv(\'",as.character(i),"\',as.is=TRUE)",sep="")))
eval(parse(text=paste("names(stock",i,") = stock",i,"[2,]",sep="")))
eval(parse(text=paste("stock",i," = stock",i,"[-(1:2),1:4]",sep="")))
eval(parse(text=paste("stock",i,"$close = as.numeric(stock",i,"$close)",sep="")))
eval(parse(text=paste("stock",i,"$volume = as.numeric(stock",i,"$volume)",sep="")))
eval(parse(text=paste("stock",i,"$amt = as.numeric(stock",i,"$amt)",sep="")))
eval(parse(text=paste("stock",i,"$time = as.POSIXct(strptime(stock",i,"$time,\'%Y-%m-%d %H:%M\'))",sep="")))
}
future10 = read.csv("future_10min.csv",colClasses=c("character","character","numeric","numeric","numeric","numeric","numeric"),col.names=c('d','t','open','high','low','close','volume'))
future10 = cbind(future10,time=paste(future10$d,future10$t))
future10$time = as.character(future10$time)
future10$time = as.POSIXct(strptime(future10$time,'%m/%d/%Y %I:%M:%S %p'))
future5 = read.csv("future_5min.csv",colClasses=c("character","character","numeric","numeric","numeric","numeric","numeric","numeric"),col.names=c('d','t','open','high','low','close','volume','volume2'))
future5 = cbind(future5,time=paste(future5$d,future5$t))
future5$time = as.character(future5$time)
future5$time = as.POSIXct(strptime(future5$time,'%m/%d/%Y %I:%M:%S %p'))
######################################
######### PROCESSING #################
######################################
## Create master data-frames   ##
# Select subset of data to be used
future10.sel <- future10[1:500,]
future5.sel <- future5[1:1000,]
# 1. eVars for Ft:
# Ft-1, Fhigh - Fclose at t-1, Flow - Fclose at t-1, max/min high - low over last 30 minutes, max of Fhigh over last 30 minutes - Fclose at t-1, Ft-1 - Ft-2, Ft-1 - Ft-day, Ft-1 - Ft-week, Ft-1 - Ft-2 * vol t-1
close <- future5.sel$close
lagClose <- getFutureLagged(future5.sel, future5.sel, as.difftime(10, unit="mins"), "close")
response <- close
fHighSubFClose.tSub1 <- getFutureLagged(future5.sel, future5.sel, as.difftime(10, unit="mins"), "high") - lagClose
fLowSubFClose.tsub1 <- getFutureLagged(future5.sel, future5.sel, as.difftime(10, unit="mins"), "low") - lagClose
highSubLow.last30 <- getFutureBestHigh(future5.sel, future5.sel, as.difftime(30, unit="mins")) - getFutureWorstLow(future5.sel, future5.sel, as.difftime(30, unit="mins"))
maxFHigh.last30.subFClose.tsub1 <- getFutureBestHigh(future5.sel, future5.sel, as.difftime(30, unit="mins")) - lagClose
f.tsub1.subF.tsub2 <- lagClose - getFutureLagged(future5.sel, future5.sel, as.difftime(15, unit="mins"), "close")
f.tsub1.subF.tsubDay <- lagClose - getFutureLagged(future5.sel, future5.sel, as.difftime(1440, unit="mins"), "close") # throws error
f.tsub1.subF.tsubWeek <- lagClose - getFutureLagged(future5.sel, future5.sel, as.difftime(7, unit="days"), "close")
f.tsub1.f.tsub2.multVol.tsub1 <- (lagClose - getFutureLagged(future5.sel, future5.sel, as.difftime(15, unit="mins"), "close")) * getFutureVolLagged(future5.sel, future5.sel, as.difftime(15, unit="mins"))
evars1 <- data.frame(response, future5.sel$time, lagClose, fHighSubFClose.tSub1, fLowSubFClose.tsub1, highSubLow.last30, maxFHigh.last30.subFClose.tsub1, f.tsub1.subF.tsub2, f.tsub1.subF.tsubDay, f.tsub1.subF.tsubWeek, f.tsub1.f.tsub2.multVol.tsub1)
# 2. eVars for Ft - Ft-1:
# Ft-1, Fhigh - Fclose at t-1, Flow - Fclose at t-1, max/min high - low over last 30 minutes, max of Fhigh over last 30 minutes - Fclose at t-1, Ft-1 - Ft-2, Ft-1 - Ft-day, Ft-1 - Ft-week, Ft-1 - Ft-2 * vol t-1
response <- close - lagClose
evars2 <- cbind(response = response, evars1[,-1])
# 3. eVars for Ft - Ft-1 > 0:
# Ft-1, Fhigh - Fclose at t-1, Flow - Fclose at t-1, max/min high - low over last 30 minutes, max of Fhigh over last 30 minutes - Fclose at t-1, Ft-1 - Ft-2, Ft-1 - Ft-day, Ft-1 - Ft-week, Ft-1 - Ft-2 * vol t-1
evars3 <- cbind(response = (response>0), evars1[,-1])
# 4. eVars for Ft - Ft-1 > 0:
# All bool >0: Fhigh - Fclose at t-1, Flow - Fclose at t-1, max/min high - low over last 30 minutes, max of Fhigh over last 30 minutes - Fclose at t-1, Ft-1 - Ft-2, Ft-1 - Ft-day, Ft-1 - Ft-week, Ft-1 - Ft-2 * vol t-1
evars4 <- data.frame(response = (response>0), future5.sel$time, (fHighSubFClose.tSub1>0), (fLowSubFClose.tsub1>0), (highSubLow.last30>0), (maxFHigh.last30.subFClose.tsub1>0), (f.tsub1.subF.tsub2>0), (f.tsub1.subF.tsubDay>0), (f.tsub1.subF.tsubWeek>0), (f.tsub1.f.tsub2.multVol.tsub1>0))
# Clean variables
rm(response, lagClose, fHighSubFClose.tSub1, fLowSubFClose.tsub1, highSubLow.last30, maxFHigh.last30.subFClose.tsub1, f.tsub1.subF.tsub2, f.tsub1.subF.tsubDay, f.tsub1.subF.tsubWeek, f.tsub1.f.tsub2.multVol.tsub1)
